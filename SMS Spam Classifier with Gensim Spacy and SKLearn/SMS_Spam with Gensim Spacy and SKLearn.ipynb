{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS Spam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZTvD4RfjjYA",
        "colab_type": "text"
      },
      "source": [
        "Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvoLx13yelW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAl_xvSyykF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.parsing.preprocessing import strip_punctuation, remove_stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHlLfUg8ypcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx2J1r4PzFSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4gIl-KxzUt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wAX3bbVzad9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Q6Lgfxjovu",
        "colab_type": "text"
      },
      "source": [
        "Loading CSV & Spacy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP5Fou0QytNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlBRKestzg8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading as latin characters as typical utf-8 wouldn't load\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFyI1l4Yj5b2",
        "colab_type": "text"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2YTHqSEzmf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao-MO2zEzuNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.rename(columns={'v1':'y', 'v2':'raw_text'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fn0OaPIzxRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spam is 1, Real is 0\n",
        "df['y'] = df['y'].astype('str').str.replace('ham', '0').replace('spam','1').astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjG5_3PV0RD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['y'] = df['y'].astype('bool')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W97Y4hAg0VCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# counting number of words that are fully capitalized\n",
        "df['num_cap_wrds'] = [sum(map(str.isupper, i.split())) for i in df['raw_text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehp8PEpXj_ks",
        "colab_type": "text"
      },
      "source": [
        "Creating a dataframe column for any SMS with more than three capitalized words \n",
        "\n",
        "(I played around with the number of fully capitalized words, and +3 words had the highest spam:ham ratio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwurSM30bp6",
        "colab_type": "code",
        "outputId": "f9e45177-fd1f-4959-f6e5-245936b7920d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "df[df['num_cap_wrds'] > 3].groupby('y').count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_text</th>\n",
              "      <th>num_cap_wrds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>179</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>202</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       raw_text  num_cap_wrds\n",
              "y                            \n",
              "False       179           179\n",
              "True        202           202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arlvuv1G0f5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['cap_plus_three'] = np.where(df['num_cap_wrds'] > 3, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSM4loLQ0kXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropping the count of the number of capitalized words, as cap_plus_three is a stronger predictor\n",
        "df = df.drop(columns='num_cap_wrds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20xKgwoN0tCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Text Processing/Feature Engineering ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwdTitsi2O3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lowercase everything, remove gensim's list of stopwords, and strip punctuation (I am removing stopwords now to improve PoS Processing)\n",
        "df['raw_text'] = df['raw_text'].apply(str.lower)\n",
        "df['raw_text'] = df['raw_text'].apply(remove_stopwords)\n",
        "df['raw_text'] = df['raw_text'].apply(strip_punctuation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxreb1wiA_jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace non unicode characters\n",
        "df['raw_text'] = df['raw_text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF65kpKHDLMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "# lemmatizing (turning words into their base)\n",
        "def preprocess(text):\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() or lemma.isnumeric() and lemma not in stopwords]\n",
        "    \n",
        "    return ' '.join(a_lemmas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC_AaPITAWts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['raw_text'] = df['raw_text'].apply(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRueSpa4ewKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parts of Speech (PoS) Tagging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "62VKy_nZSzMS",
        "colab": {}
      },
      "source": [
        "# Takes text, returns number of proper nouns, nouns, pronounces, (pre)determiners, adverbs, verbs, and numbers in text\n",
        "def pos_count(text):\n",
        "    doc = nlp(text)\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    return [pos.count('PROPN'), pos.count('NOUN'), pos.count('PRON'),  pos.count('DET'), pos.count('ADV'), pos.count('VERB'), pos.count('NUM')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OVTC38kZZm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply above function to dataframe\n",
        "df['PoS_Vals'] = df['raw_text'].apply(pos_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j_3mtjtgwMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign values to dataframe\n",
        "df[['PROPN', 'NOUN', 'PRON', 'DET', 'ADV', 'VERB', 'NUM']] = pd.DataFrame(df.PoS_Vals.tolist(), index= df.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRGXu0n9b1WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropping original column-lists for PoS results as they are no longer necessary\n",
        "df = df.drop(columns='PoS_Vals')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXV7l00Cl83L",
        "colab_type": "text"
      },
      "source": [
        "Term Frequency - Inverse Document Freqeuncy Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk6fBnH0otVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I remove stopwords using two separate libraries to filter out a large pool of unnessesary words. This removes meaningless values from the dataset\n",
        "vectorizer = TfidfVectorizer(use_idf=True, strip_accents='ascii', stop_words='english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds-Udlkrb9C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_text = vectorizer.fit_transform(df['raw_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAHJDDgorvb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_df = pd.DataFrame(tfidf_text.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCmCOCWvqhSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([df, tfidf_df], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JNdUR5fxn_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df[['y']]\n",
        "X = df.drop(columns=['raw_text', 'y'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMvTfTz_xRX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCjAyHzwyHUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIceRzeAyKUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6klXjAsyNw1",
        "colab_type": "code",
        "outputId": "dd183f17-bca1-4828-9a17-d6d4b413b1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.fit(X_train, np.ravel(y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esj0geCByT2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg83j0JKyY8q",
        "colab_type": "code",
        "outputId": "58ba6a40-349d-4d6d-f496-6bd070fddf34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "print(classification_report(y_test, y_pred, digits=3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False      0.955     0.997     0.976       965\n",
            "        True      0.972     0.700     0.814       150\n",
            "\n",
            "    accuracy                          0.957      1115\n",
            "   macro avg      0.964     0.848     0.895      1115\n",
            "weighted avg      0.958     0.957     0.954      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27o-BdO-4Zm6",
        "colab_type": "text"
      },
      "source": [
        "95% accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A339QpLqy0Gp",
        "colab_type": "code",
        "outputId": "3fc7c710-8381-4bba-896e-327f012eaf71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[962   3]\n",
            " [ 45 105]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra73_ho5zKnv",
        "colab_type": "code",
        "outputId": "1c523c77-3542-4529-f517-ec5b6d0e9f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(f\"True Positive: {results[0][0]} \\n False Positive: {results[0][1]} \\\n",
        "      \\n False Negative: {results[1][0]} \\n True Negative: {results[1][1]}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positive: 962 \n",
            " False Positive: 3       \n",
            " False Negative: 45 \n",
            " True Negative: 105\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}